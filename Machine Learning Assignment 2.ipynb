{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae7c0080-3d77-4cba-bb1e-b5a7db7f1726",
   "metadata": {},
   "source": [
    "## Causal Inference and Machine Learning\n",
    "\n",
    "You will revisit the exercise in Week 6 about estimating the effect of the mindset intervention (Athey and Wager, 2019). Download the synthetic data `synthetic_mindset_data.csv` from our Week 6 module. More data descriptions are available in our computer lab exercise questions.\n",
    "\n",
    "In this assignment, you are asked to implement the machine learning estimation of the average treatment effect over all schools using various machine learning estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd8ec18-285b-42a8-9dbb-2b28ef64afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, AdaBoostClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow\n",
    "import random as python_random\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from tensorflow.keras import layers, optimizers, regularizers\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe8c14b-b23a-423a-9305-4bfd4b1fcae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and clean the data as in the computer lab\n",
    "\n",
    "data_original = pd.read_csv('synthetic_mindset_data.csv').rename(columns={'Z':'D'})\n",
    "n = data_original.shape[0]\n",
    "data = pd.get_dummies(data_original,columns=['C1','XC'])\n",
    "X = data.iloc[:,3:]\n",
    "Y = data.Y\n",
    "D = data.D\n",
    "schoolid = data.schoolid-1 # then the schoolid starts from 0\n",
    "K = 76 # Number of schools \n",
    "delta_school_dml=np.zeros((K,)) # Initialize the school-specific estimates\n",
    "\n",
    "# Initialize the dict objects to store estimates.\n",
    "\n",
    "delta_debias={}\n",
    "delta_raw={}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4331f42a-fa0e-4055-87c7-9777c33fd61b",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Estimate the average treatment effects by using the random forest (of 200 trees) algorithm including a variable selection as proposed by Athey and Wager (2019). Note that you can extract the feature importance directly from sklearn. You should always use the debiased ML estimator across clusters, as in the Week 6 computer lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9527db-e1e6-4f8d-b102-a97619785e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "\n",
    "n_estimators = 500 # Number of trees to estimate \n",
    "\n",
    "rf_mu = RandomForestRegressor(\n",
    "    n_estimators=n_estimators,     \n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "rf_p = RandomForestClassifier(\n",
    "    n_estimators=n_estimators,     \n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Estimate the treatment effect for each school\n",
    "\n",
    "for k in tqdm(range(K)):\n",
    "    \n",
    "    # Fit pilot random forests\n",
    "    \n",
    "    selector = (schoolid == k)\n",
    "    rf_mu.fit(X[~selector], Y[~selector])\n",
    "    rf_p.fit(X[~selector], D[~selector])\n",
    "    \n",
    "    # Select the subset of variables with importance higher than their average\n",
    "    # Repeat this procedure for each school\n",
    "    \n",
    "    importances_mu = rf_mu.feature_importances_\n",
    "    importances_p = rf_p.feature_importances_\n",
    "    selected_features_mu = X.columns[importances_mu > np.mean(importances_mu)]\n",
    "    selected_features_p = X.columns[importances_p > np.mean(importances_p)]\n",
    "    \n",
    "    # Fit random forests again, using only the selected variables\n",
    "    \n",
    "    rf_mu.fit(X.loc[~selector, selected_features_mu], Y[~selector])\n",
    "    rf_p.fit(X.loc[~selector, selected_features_p], D[~selector])\n",
    "    Y_res = Y[selector].values-rf_mu.predict(X.loc[selector, selected_features_mu])\n",
    "    v = D[selector].values-rf_p.predict_proba(X.loc[selector, selected_features_p])[:,1]\n",
    "    \n",
    "    # Store the estimated treatment effect for each school\n",
    "    \n",
    "    delta_school_dml[k] = np.mean(Y_res*v)/np.mean(v**2) # Debiased estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d954f29a-4b50-4c3a-8b1c-d253fbca1f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your estimate here\n",
    "\n",
    "delta_debias['RF'] = np.mean(delta_school_dml)\n",
    "print(delta_debias['RF'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64441c4a-0e41-45e5-8ff8-e41ad813b8d1",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Estimate the average treatment effects by using:\n",
    "\n",
    "- Least-squares boosting for estimation of $\\mu^{(0)}(x) = E[Y^{(0)}_i|X_i = x]$, using a learning rate of no more than 0.1.\n",
    "- AdaBoost for estiamtion of $p(x) = P(D_i = 1|X_i = x)$\n",
    "\n",
    "You don't need to consider variable selection. Keep the number of bases to no more than 500 for each model.\n",
    "\n",
    "You should always use the debiased ML estimator across clusters, as in the Week 6 computer lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b1ffd2-45a2-4095-8ba3-d4ea285ac818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the models\n",
    "\n",
    "n_estimators = 500 \n",
    "\n",
    "gb_mu = GradientBoostingRegressor(\n",
    "    n_estimators=n_estimators, \n",
    "    learning_rate=0.1, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "ada_p = AdaBoostClassifier(\n",
    "    n_estimators=n_estimators, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Estimate the treatment effect for each school\n",
    "\n",
    "for k in tqdm(range(K)):\n",
    "    \n",
    "    # Selector for current school\n",
    "    \n",
    "    selector = (schoolid == k)\n",
    "\n",
    "    # Fit models\n",
    "    \n",
    "    gb_mu.fit(X[~selector], Y[~selector])\n",
    "    ada_p.fit(X[~selector], D[~selector])\n",
    "\n",
    "    # Predict outcomes and treatment probabilities for each school\n",
    "    \n",
    "    Y_res = Y[selector].values-gb_mu.predict(X[selector])\n",
    "    v = D[selector].values-ada_p.predict_proba(X[selector])[:,1]\n",
    "    \n",
    "    # Store the estimated treatment effect for each school\n",
    "    \n",
    "    delta_school_dml[k] = np.mean(Y_res*v)/np.mean(D[selector].values*v) # Debiased estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1607779e-67e4-4e47-81dd-38318c33e00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimates\n",
    "\n",
    "delta_debias['Boost']=np.mean(delta_school_dml)\n",
    "print(delta_debias['Boost'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53f80bc-04b4-49c4-a4ca-d1b77c283ec9",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Estimate the average treatment effects by estimating $\\mu^{(0)}(x) = E[Y^{(0)}_i|X_i = x]$ and $p(x) = P(D_i = 1|X_i = x)$ with two separate neural networks.\n",
    "\n",
    "Use no more than 6 neurons in each hidden layer, no more than 3 hidden layers, and a weight decay penalty parameter no larger than 0.1. Use ReLU for deep neural nets but sigmoid for shallow nets.\n",
    "\n",
    "You should always use the debiased ML estimator across clusters, as in the Week 6 computer lab.\n",
    "\n",
    "We perform a Grid Search to find the optimal number of hidden layers and units for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eac634a-75a7-45bc-ab83-882c434ef9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "\n",
    "X_train, X_val, Y_train, Y_val, D_train, D_val = train_test_split(X, Y, D, test_size=0.2, random_state=42)\n",
    "\n",
    "def build_model(hp):\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    # Choose the number of hidden layers (1, 2, or 3)\n",
    "    \n",
    "    num_hidden_layers = hp.Int('num_hidden_layers', 1, 3)\n",
    "\n",
    "    for i in range(num_hidden_layers):\n",
    "        \n",
    "        # Choose the number of neurons (1 to 6)\n",
    "        \n",
    "        units = hp.Int(f'units_layer_{i}', min_value=1, max_value=6)\n",
    "\n",
    "        # Add hidden layers with ReLU activation and L2 regularization\n",
    "        \n",
    "        model.add(layers.Dense(units=units, activation='relu',\n",
    "                               kernel_regularizer=regularizers.l2(0.1)))\n",
    "\n",
    "    # Output layer with linear activation\n",
    "    \n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "    # Compile the model\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-3, 1e-4])),\n",
    "        loss='mean_absolute_error',\n",
    "        metrics=['mean_absolute_error']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Set up the RandomSearch tuner\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_mean_absolute_error',\n",
    "    max_trials=10,  \n",
    "    executions_per_trial=3,  \n",
    "    directory='my_dir',\n",
    "    project_name='hidden_layer_tuning'\n",
    ")\n",
    "\n",
    "# Perform the search\n",
    "\n",
    "tuner.search(\n",
    "    X_train, Y_train,\n",
    "    epochs=20,  \n",
    "    batch_size=32,  \n",
    "    validation_data=(X_val, Y_val),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best hyperparameters\")\n",
    "print(f\"Number of hidden layers: {best_hps.get('num_hidden_layers')}\")\n",
    "for i in range(best_hps.get('num_hidden_layers')):\n",
    "    print(f\"Units in layer {i}: {best_hps.get(f'units_layer_{i}')}\")\n",
    "print(f\"Learning rate: {best_hps.get('learning_rate')}\")\n",
    "\n",
    "# Build the model with the best hyperparameters\n",
    "\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train the best model\n",
    "\n",
    "history = best_model.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=50,  \n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, Y_val),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluate the best model on validation data\n",
    "\n",
    "val_loss, val_mae = best_model.evaluate(X_val, Y_val, verbose=0)\n",
    "print(f\"Validation Loss: {val_loss}, Validation MAE: {val_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b933929e-ebbb-47a5-acd6-16448a350977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the models\n",
    "\n",
    "def create_model(input_dim, output_activation, loss, weight_decay=0.1, units_layer_0=4):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=units_layer_0, activation='relu', input_dim=input_dim, kernel_regularizer=l2(weight_decay)))\n",
    "    model.add(Dense(units=1, activation=output_activation, kernel_regularizer=l2(weight_decay)))\n",
    "    model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Models for mu0 and p\n",
    "\n",
    "model_mu = create_model(input_dim=X.shape[1], output_activation='linear', loss='mean_squared_error', weight_decay=0.1, units_layer_0=4)\n",
    "model_p = create_model(input_dim=X.shape[1], output_activation='sigmoid', loss='binary_crossentropy', weight_decay=0.1, units_layer_0=4)\n",
    "\n",
    "delta_school_dml = np.zeros(K)\n",
    "\n",
    "# Estimate the treatment effect for each school\n",
    "\n",
    "for k in tqdm(range(K)):\n",
    "    \n",
    "    # Selector for the current school\n",
    "    \n",
    "    selector = (schoolid == k)\n",
    "\n",
    "    # Fit models using training data excluding the current school\n",
    "    \n",
    "    model_mu.fit(X[~selector], Y[~selector], epochs=50, batch_size=32, verbose=0)\n",
    "    model_p.fit(X[~selector], D[~selector], epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "    # Predict outcomes and treatment probabilities\n",
    "    \n",
    "    Y_res = Y[selector].values - model_mu.predict(X[selector]).flatten()\n",
    "    v = D[selector].values - model_p.predict(X[selector]).flatten()\n",
    "\n",
    "    # Store the estimated treatment effect for each school\n",
    "    \n",
    "    delta_school_dml[k] = np.mean(Y_res * v) / np.mean(D[selector].values * v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ecadad-5ec0-409d-8c2a-64c3b0b0aee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your estimate here\n",
    "\n",
    "delta_debias['NN'] = np.mean(delta_school_dml)\n",
    "print(delta_debias['NN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b0b550-fbe2-4f80-8d6f-a290a85ce9d8",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Repeat Exercise 1, but now _without_ using debiased methods:\n",
    "\n",
    "- Estimate the regression functions $\\mu^{(0)}(x) = E[Y^{(0)}_i|X_i = x]$ and $p(x) = P(D_i = 1|X_i = x)$ by pooling ALL observations.\n",
    "- Estimate the treatment effects for each school.\n",
    "- Average the estimated treatment effects over schools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1cc56f-a6ac-44a7-92ac-82eb16e9f838",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 200\n",
    "\n",
    "rf_mu = RandomForestRegressor(\n",
    "    n_estimators=n_estimators,     \n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "rf_p = RandomForestClassifier(\n",
    "    n_estimators=n_estimators,     \n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "delta_school_dml = np.zeros(K)\n",
    "\n",
    "# Fit the models using observations over all schools\n",
    "\n",
    "rf_mu.fit(X, Y)\n",
    "rf_p.fit(X, D)\n",
    "\n",
    "# Select the subset of variables with importance higher than their average\n",
    "\n",
    "importances_mu = rf_mu.feature_importances_\n",
    "importances_p = rf_p.feature_importances_\n",
    "\n",
    "selected_features_mu = X.columns[importances_mu > np.mean(importances_mu)]\n",
    "selected_features_p = X.columns[importances_p > np.mean(importances_p)]\n",
    "\n",
    "# Fit random forests again, using only the selected variables\n",
    "\n",
    "rf_mu.fit(X.loc[:,selected_features_mu], Y)\n",
    "rf_p.fit(X.loc[:,selected_features_p], D)\n",
    "\n",
    "\n",
    "# Estimate the treatment effect for each school\n",
    "\n",
    "for k in tqdm(range(K)):\n",
    "    \n",
    "    selector = (schoolid == k)\n",
    "\n",
    "    # Calculate the treatment effect\n",
    "    \n",
    "    Y_res = Y[selector].values-rf_mu.predict(X.loc[selector, selected_features_mu])\n",
    "    D_k = D[selector].values\n",
    "    \n",
    "    \n",
    "    # Store the estimated treatment effect for each school\n",
    "    \n",
    "    delta_school_dml[k] = np.mean(Y_res * D_k) / np.mean(D_k**2) # Conventional LS estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349c2ce9-1783-46b8-90f8-6de2214f7f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your estimate here\n",
    "\n",
    "delta_raw['RF'] = np.mean(delta_school_dml)\n",
    "print(delta_raw['RF'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipjupyter",
   "language": "python",
   "name": "pipjupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
